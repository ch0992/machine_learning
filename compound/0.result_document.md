

---
### 1. Abstract

1) solution direction
2) 1.description for cmpd_clf_manual.ipynb
3) 2.description for cmpd_clf_gridsearch.ipynb
4) 3.evaluation_muiltiple_models.ipynb
5) result

먼저 본 문서에서는 금번 과제간 발생한 산출물에 대해 위의 세가지 항목으로 나누어 설명하고자 합니다.
**solution direction**은 문제해결에 대한 방향을 설명하고자 하며 과제에 임하는 지원자의 배경지식 및 업무적합도를 보다 분명히 파악하기 위한 정보를 제공하고자 문제해결간 사고가 어떻게 확장되어 갔는지에 대해 좀더 풀어서 설명드리고자 합니다. 
**1.description for cmpd_clf_manual.ipynb** 에서는 해당 산출물에 대한 전반적인 설명으로서 solution direction에 언급된 방향을 바탕으로 무엇을 얻고자 했고 또 어떠한 것을 해당 노트북에 정의된 코드로 얻을 수 있었는지에 대해 설명합니다.
**2.description for cmpd_clf_gridsearch.ipynb**는 위와 동일한 내용에 대해 기술합니다.
**3.eveluation_multiple_models.ipynb** 에서는 네개의 머신러닝 분류 알고리즘별 성능을 평가하게된 배경과 결과에 대해 간단하게 리뷰합니다.




</br>
#### 1) solution diriection
- 배경지식
  &nbsp;
  &nbsp;저는 Devops, MLops 직무 지원자로 Languages for Development Application (Node.js, Java) 및 Cloud Resources, 
  &nbsp;Kubernetes 등을 최근에 많이 다루어 보았습니다. Machine Learning 혹은 Deep Learning 등은 관심은 있었으나 정확한 개념에 
  &nbsp;대한 이해 및 관련 코드 또는 오픈소스는 다루어 본적이 없는 상태에서 해당 과제를 시작했습니다.
  </br>


- 사고확장 순서 및 방향
  &nbsp;
  * **과제 요구사항은 과적합을 고려한 성능개선을 위한 모델 적용 및 튜닝으로 이해했습니다.**
  </br>
  1) 먼저 주어진 과제의 Demo에 정의된 Random Forest와 Compound 데이터 셋등을 이해하는데서 부터 시작했습니다. Random Forest(이하 RF)에 대해서 자세히 언급하진 않겠습니다. 다만 RF가 데이터셋에서 무작위로 데이터를 뽑아 의사결정나무를 정의하기 떄문에 과적합을(편향된 데이터로 학습하여 범용적인 사용에 적합하지 않은 상태) 방지하기에 용이한 앙상블이라고 이해하였으나 Demo의 Hint 등에서 언급하신 것과 Rf, Morgan FingerPrint가 과적합을 해소하지 못하며 실제로 그렇다는 데서 착안하여 RF에서 과적합을 방지하기 위한 방법으로 **하이퍼 파라미터 튜닝**이 있다는 것을 발견하고 이에 대한 개념 및 이를 위한 소스코드를 검색한 결과를 토대로 적용하는 단계에 이르렀습니다.
  </br>
  2) 하이퍼 파라미터 튜닝을 위해 RF의 파라미터에 정의되는 Input 값을 변경하면서 이를 표로 산출 및 그래프화 하여 과적합 되는 분기점을 찾고자 하였으며 그로인해 과적합이 되지 않는 파라미터 정의 값을 단계별로 찾고자 하였습니다. 만약 GridSearch를 한다고 하더라도 제시되는 최초 기준값의 적절성을 판단하기 어렵기 때문에 먼저 Manual로 각 파라미터별 과적합 여부를 판단하는 함수를 정의하고 이를 통해 GridSearch에 정의되는 파라미터 기준값을 최적화 하고자 하였습니다.
  </br>
  3) 개인적으로 정의한 함수를 통한 하이퍼 파라미터 튜닝으로는 다양한 입력에 대한 시도가 어렵기 떄문에 최적화된 파라미터 튜닝은 어렵다고 판단하여 이를 위한 방법을 찾던 도중 GridSearch에 대해 인지 및 이해하게 되었고 이를 적용하게 되었습니다.
  GridSearch를 통해 기준값을 중심으로 보다 효과적인 파라미터 튜닝이 가능하게 되었으나 하이퍼 파라미터 튜닝으로는 Casual Score의 성능이슈를 극복하기 어렵다는 결론에 다다르게 되었습니다.
  </br>
  4) 따라서 Demo의 Hint에 기술되어 있는 내용대로 자연스럽게 Deeplearning으로 눈을 돌리게 되었고 최근 AI 신약분야에서 주로 CNN, RNN으로 모델을 만들어 더 나아가서는 트랜스포머로 학습하는 것이 많이 쓰인다는 것을 알게되었습니다. **아마도 최종적인 정답은 적은 데이터로 의미있는 결과를 내는 트랜스포머를 사용하는 것**이겠지만 기존에 학습된 트랜스포머를 사용하지 않는 이상 제가 개인적으로 사용하기 어렵다는 것을 알고 CNN 모델개발 및 학습을 진행코자 하였으나 현재 제 수준으로는 단기간에 딥러닝 모델 개발이 어렵다는 생각이 들었습니다. 따라서 딥러닝 모델 개발중에 데이터셋 정의 및 파라미터 구성단계에서 이를 포기하고 해당 데이터에 RF보다 효과적인
  머신러닝 분류 알고리즘이 존재하는지 확인하기 위해 kaggle에서 이를 비교하는 라이브러리를 참조하여 성능을 비교해보았으나
  RF가 가장 우수한 성능을 보였습니다.
  </br>
  5) 결과적으로 Demo에서 구현해놓은 Casual Score 및 AUC PRC, AUC ROC, Logloss 등은 의미있는 개선을 하지는 못했습니다.
     하이퍼 파라미터 튜닝으로는 해당 요구사항 충족이 어려웠으며 아마도 딥러닝을 사용하거나 데이터셋에 변화를 주어야 할 것으로
     예상이 됩니다. 하지만 위에 언급한대로 딥러닝 사용은 지금의 제 수준으로는 단기간에 적용하기는 어려움이 크다고 판단 하였습니다.


</br>

#### 2) 1.description for cmpd_clf_manual.ipynb
  1) 하이퍼 파리미터 튜닝을 위해 튜닝에 대한 기본 이해 및 코드 적용을 하는 과정이었으며 처음에는 의도치 않았으나 이후에 GridSearch를 적용하는 과정에서 해당 작업이 GridSearch 파라미터 값에 중요한 기준점을 제공하다는 생각을 하게 되었습니다.
  해당 프로세스를 통해 성능지표 개선 뿐 아니라 과적합에 대해서 보다 자세히 시각화하여 확인하고 개선할 수 있었습니다.
  </br>
  2) 하이퍼 파라미터 튜닝을 위한 함수 정의를 코드화 하면서 다음의 머신러닝 분류 모델의 성능평가 지표에 대해 이해하게 되었습니다.
     - Accuracy
     - Recall
     - Precision
     - F1 Score
  </br>    
  3)  해당 과정을 통해 하이퍼 파라미터 튜닝이 무엇인지 어떻게 해야하는지 좀더 명확히 이해할 수 있었고 제가 지원한 과업인 MLops에서 모델 성능 개선을 위해 어떤 작업이 필요한지 그리고 해당작업을 자동화하기 위해서는 어떤 것들이 필요한지 짐작하고 찾아볼 수 있는 좋은 계기가 되었습니다.

</br>

#### 3) 2.description for cmpd_clf_gridsearch.ipynb
  1) 위에서 작성한 주피터 노트북에 정의된 코드로 하이퍼 파라미터 튜닝을 하는 것은 매우 비효율적이었습니다. 튜닝간에 학습결과를 보기위해서는 단계별로 진행해야하지만 또 값을 하나씩 바꿔서 테스트하는 주기가 매우 길고 느렸습니다. 이를 보완하기 위해 구글링을 하다가 Gridsearch 라는 것을 알게되었고 위에 언급한 문제를 해결할 수 있다는 것을 알게되었습니다.
  </br>    
  2) 결과적으로 GridSearch를 통해 하이퍼 파라미터 튜닝이 보다 간편하게 되었으며 파라미터 값에 대한 범위를 설정하는 것이
     무엇보다 매우 중요하다는 것을 깨닫게 되었습니다.


</br>

#### 4) 3.description for eveluation_multiple_models.ipynb
  1) 위의 **사고확장 순서 및 방향** 에서 언급한대로 딥러닝 적용을 시도하였으나 앙상블 적용만큼 비교적 간단하지 않아 해당 과정은
     생략하게 되었습니다. 하지만 다른 분류 알고리즘으로도 더 좋은 성능을 낼 수 있을지도 모른다는 생각에 Kaggle에서 앞서 설명한
     동작을 라이브러리로 구현한 것을 찾아 적용 및 분류 알고리즘별로 평가하게 되었습니다.     
  </br>
  2) 결론적으로 Accuracy, Recall, Precision, F1 Score 등의 지표를 기반으로 평가한 결과 RF가 가장 우수한 성능을 보였습니다.

</br>

#### 5) Result
개인적으로 해당 과제가 처음에는 막막했지만 진행하는 과정에서 머신러닝 및 딥러닝에 대해서 많이 이해하게 되는 계기가 되었습니다.
결과적으로는 과제 출제자가 기대한 수준에 못미치는 답을 드리는 것 같아 매우 아쉬운 마음이 크지만 저의 현재 수준을 정확히 알리는 것도
해당 포지션에 적합한 사람을 뽑는 것에 있어 정확한 정보를 드리는 매우 중요한 일이라는 생각이 들어 불필요할 수도 있으나 위의 글들을 통해
어떻게 사고했고 현재의 결론을 얻게 되었는지 설명을 드리고자 했습니다.
처음에는 MLops/Devops 직군에서 해당 과제가 과연 필요한 일일까 하는 의문을 가졌으나 진행과정에서 Endpoint인 Application 개발에 대해 이해하는 DevOps가 애초에 DevOps가 의도한 개발생산성 및 운영연계성 증진이라는 목표에 걸맞는 역할을 할 수 있는 것처럼
모델개발 및 이를 고도화 하는 과정을 이해하는 사람이 보다 넓은 의미에서 MLops에 걸맞는 사람이라는 생각이 들었습니다.
시니어를 뽑는 포지션이니 만큼 해당 과제의 당락을 감히 예상하기 어려우나 진심으로 해당 과제를 통해 많이 배우는 계기가 되어 감사드린다는
말을 끝으로 해당 Document를 마무리하고자 합니다.
감사합니다.
